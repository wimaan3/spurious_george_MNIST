{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4ae8355-a733-4bd7-85c0-928631bacb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the process...\n",
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████| 48004/48004 [00:04<00:00, 11385.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset initialized!\n",
      "Initializing the model...\n",
      "Model initialized!\n",
      "Starting ERM training...\n",
      "ERM training complete!\n",
      "Generating outputs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|████████████████████████████████████████████████| 1501/1501 [00:01<00:00, 816.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs generated!\n",
      "Performing clustering...\n",
      "Clustering complete!\n",
      "Start group-balanced training...\n",
      "Group-balanced training complete!\n",
      "Evaluating the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating batches:   1%|▋                                                 | 22/1501 [00:00<00:07, 202.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real digit: 2, Predicted digit: 2\n",
      "Real digit: 0, Predicted digit: 0\n",
      "Real digit: 2, Predicted digit: 2\n",
      "Real digit: 0, Predicted digit: 0\n",
      "Real digit: 4, Predicted digit: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating batches: 100%|████████████████████████████████████████████████| 1501/1501 [00:02<00:00, 710.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 99.21%\n",
      "0.992125656195317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Make sure to download the necessary libraries: torch, spuco, pandas, tqdm\n",
    "# We need to import os, a built-in Python module that allows us to \n",
    "# interact with the operating system, to handle files and directories\n",
    "import os\n",
    "\n",
    "# import the Pandas library for data manipulation and analysis\n",
    "import pandas as pd\n",
    "# import the core library of PyTorch the machine learning framework\n",
    "# for tensor operations and neural networks\n",
    "import torch\n",
    "# Here we are importing api references and classes from the spuco package \n",
    "# The datasets api reference contains the classes we need to initialize our dataset\n",
    "# SpuCoMNIST initializes the dataset of images from the MNIST dataset\n",
    "# where each image has spurious features added \n",
    "# The SpuriousFeatureDifficulty is the level of the spurious feature in each image\n",
    "from spuco.datasets import SpuCoMNIST, SpuriousFeatureDifficulty\n",
    "# The models api reference contains the function to create a machine learning SpuCoModel\n",
    "from spuco.models import model_factory\n",
    "# The robust_train api reference contains the Empirical Risk Minimization algorithim\n",
    "from spuco.robust_train.erm import ERM\n",
    "# The optim module in PyTorch used to adjust the weights and biases of the NN to minimize loss\n",
    "import torch.optim as optim\n",
    "\n",
    "# We need tgdm for animations\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"Starting the process...\")\n",
    "print(\"Loading dataset...\")\n",
    "# Initialize the dataset \n",
    "# Other missing parameters will be set as default automatically\n",
    "dataset = SpuCoMNIST(\n",
    "    # Initialize the dataset in a folder called data\n",
    "    root=\"./data\", \n",
    "    # Set the spurious feature to a large magnitude\n",
    "    spurious_feature_difficulty=SpuriousFeatureDifficulty.MAGNITUDE_LARGE,\n",
    "    # We map the digits we want to classify into binary groups simplifying the problem\n",
    "    classes=[[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]],\n",
    "    # Here we are making the spurious correlation strength even more difficult\n",
    "    spurious_correlation_strength=0.9,  \n",
    "    # Here we are setting the dataset to load the training set\n",
    "    # This lets the model adjust its weights based on the errors made\n",
    "    split=\"train\",  \n",
    "    # Here we let the dataset be downloaded automatically if not available locally\n",
    "    download=True\n",
    ")\n",
    "\n",
    "# Here we call the initialize method to initialize the dataset we defined above\n",
    "dataset.initialize() \n",
    "print(\"Dataset initialized!\")\n",
    "\n",
    "\n",
    "# We are building a leNet CNN which is best for the MNIST dataset\n",
    "print(\"Initializing the model...\")\n",
    "model = model_factory(arch= \"lenet\", \n",
    "                      # We need the dataset to be returned in a tuple \n",
    "                      # first index for the image and second for the label\n",
    "                      input_shape= dataset[0][0].shape, \n",
    "                      # We have five classes because we set the classes \n",
    "                      # of 10 digits to be split in two\n",
    "                      num_classes= 10)\n",
    "print(\"Model initialized!\")\n",
    "\n",
    "# Here we are creating the optimizer\n",
    "# Adam is for Adaptive Moment Estimation\n",
    "# We speed up training with momentum \n",
    "# And we adjust the learning rate for each parameter\n",
    "# We use a learning rate of 0.001 as it is a common choice for deep learning\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Here we are establishing the model training set up using the model we defined\n",
    "# Our dataset, a batch size of 32 which is good for how many samples will be passed at once per iteration\n",
    "# And we use our optimizer from before\n",
    "# We use 10 epochs for how many times the model will go through training\n",
    "trained_model = ERM(model= model, trainset= dataset, batch_size= 32, optimizer= optimizer, num_epochs= 5)  \n",
    "\n",
    "# Here we start training the model\n",
    "print(\"Starting ERM training...\")\n",
    "\n",
    "trained_model.train()\n",
    "\n",
    "print(\"ERM training complete!\")\n",
    "\n",
    "# Now that the model is trained we set it to evaluation mode\n",
    "# This disables dropout behavior which is only for training\n",
    "# Now we can pass data through it to test the model\n",
    "model.eval()\n",
    "\n",
    "# Now create a dataloader to load the dataset into batches so you can\n",
    "# pass the data efficiently\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# We set shuffle to false\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# For each batch of data perform a forward pass through the model \n",
    "# To collect outputs when passing data through the NN we trained\n",
    "# Disable gradients since you don't need it for inference mode since\n",
    "# we are just making predictions\n",
    "\n",
    "# Here we are initializing an array of outputs \n",
    "outputs = []\n",
    "print(\"Generating outputs...\")\n",
    "\n",
    "# The outptus that will be produced are called logits which are raw scores\n",
    "# so the model will output a tensor of size 5\n",
    "\n",
    "# We turn off gradients to reduce memory-use since we are not training \n",
    "with torch.no_grad():\n",
    "    # We need to iterate over each batch in the data loader\n",
    "    for batch in tqdm(dataloader, desc=\"Processing batches\"):\n",
    "        # We don't want the labels in the batches only the images of the dataset\n",
    "        inputs, _ = batch\n",
    "        # We then pass the input into the model and set the logits to the batch_outputs\n",
    "        batch_outputs = model(inputs)\n",
    "        # We then append the logits to the output array\n",
    "        outputs.append(batch_outputs)\n",
    "\n",
    "# Here we concatenate the outputs into a single tensor using the dim=0 so we can input\n",
    "# them into the cluster function from spuco\n",
    "outputs = torch.cat(outputs, dim=0)\n",
    "print(\"Outputs generated!\")\n",
    "\n",
    "# Import the cluster class from the group_inference api reference\n",
    "from spuco.group_inference import Cluster\n",
    "\n",
    "# We need to organize the outputs into clusters so we can detect biases and use group-balanced training\n",
    "print(\"Performing clustering...\")\n",
    "\n",
    "cluster = Cluster(outputs, num_clusters=5)\n",
    "\n",
    "# We need the cluster to be in a group partition for it to work in the group balance batch ERM function\n",
    "group_partition = cluster.infer_groups()\n",
    "\n",
    "print(\"Clustering complete!\")\n",
    "\n",
    "\n",
    "# We use another optimizer for retraining\n",
    "optimizer_new = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Here we import the GroupBalanceBatchERM from the robust_train API Reference\n",
    "from spuco.robust_train import GroupBalanceBatchERM\n",
    "\n",
    "# This training will ensure each group is represented equally\n",
    "# This is useful for the spurious correlations problem since there are groups\n",
    "# in the data that the model relies on for prediction\n",
    "balance = GroupBalanceBatchERM(model= model, trainset= dataset, \n",
    "                                           group_partition= group_partition,\n",
    "                                           batch_size= 32,\n",
    "                                           optimizer= optimizer_new,\n",
    "                                           num_epochs= 10)\n",
    "\n",
    "print(\"Start group-balanced training...\")\n",
    "\n",
    "balance.train()\n",
    "\n",
    "print(\"Group-balanced training complete!\")\n",
    "\n",
    "\n",
    "def evaluate_model(model, dataset):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=False)  # Create a DataLoader to provide the data\n",
    "    # here we have a counter to count how many correct predictions made\n",
    "    correct = 0\n",
    "    # here is a counter for the total amount of samples processed\n",
    "    total = 0\n",
    "\n",
    "    # Here we do another forward pass\n",
    "    print(\"Evaluating the model...\")\n",
    "    with torch.no_grad():  # Disable gradients for evaluation\n",
    "        # We want to track how many truth/predictions we printed\n",
    "        printed_count = 0\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating batches\"):\n",
    "            inputs, labels = batch  # Get inputs and true labels\n",
    "            outputs = model(inputs)  # Get model predictions (logits)\n",
    "            _, predicted = torch.max(outputs.data, 1)  # Convert logits to predicted classes\n",
    "            \n",
    "            # Print truth and predictions for the first 5 samples\n",
    "            for truth, pred in zip(labels, predicted):\n",
    "                if printed_count < 5:\n",
    "                    print(f\"Real digit: {truth.item()}, Predicted digit: {pred.item()}\")\n",
    "                    printed_count += 1\n",
    "        \n",
    "            total += labels.size(0)  # Increment the total number of samples\n",
    "            correct += (predicted == labels).sum().item()  # Count correct predictions\n",
    "    \n",
    "    accuracy = correct / total  # calculate accuracy\n",
    "    return accuracy\n",
    "\n",
    "# Now call the function to eval the accuracy\n",
    "accuracy = evaluate_model(model, dataset)\n",
    "\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010fb20f-819a-48ed-b480-e28be4019b9e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
